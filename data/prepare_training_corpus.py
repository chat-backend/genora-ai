# prepare_training_corpus.py
import os, re, unicodedata, logging, argparse, json
from dotenv import load_dotenv
from openai import OpenAI
from difflib import SequenceMatcher

# -------------------------------
# Logging
# -------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("GenoraAI.Corpus")

# -------------------------------
# OpenAI client
# -------------------------------
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("OPENAI_API_KEY chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh.")
client = OpenAI(api_key=api_key)

# -------------------------------
# 1. LÃ m sáº¡ch vÄƒn báº£n
# -------------------------------
def clean_text(raw: str) -> str:
    text = unicodedata.normalize("NFKC", raw)
    text = "".join(ch for ch in text if unicodedata.category(ch)[0] != "C")
    text = re.sub(r"[^\w\s.,;:!?()\-\n]", "", text)
    return re.sub(r"\s+", " ", text).strip()

# -------------------------------
# 2. Chia nhá» vÄƒn báº£n
# -------------------------------
def split_text(raw: str, max_words=1200):
    words = raw.split()
    return [" ".join(words[i:i+max_words]) for i in range(0, len(words), max_words)]

# -------------------------------
# 3. Sinh chá»§ Ä‘á» + tÃ³m Ã½ + bullet cho Ä‘oáº¡n
# -------------------------------
def process_chunk(chunk: str) -> str:
    prompt = f"""
Báº¡n lÃ  Genora AI â€“ trá»£ lÃ½ Pháº­t há»c chuyÃªn sÃ¢u, trang nghiÃªm.
HÃ£y Ä‘á»c Ä‘oáº¡n vÄƒn sau vÃ :
- Äáº·t má»™t tiÃªu Ä‘á» ngáº¯n gá»n, trang nghiÃªm, pháº£n Ã¡nh tinh tháº§n Pháº­t há»c.
- Viáº¿t má»™t cÃ¢u tÃ³m Ã½ chÃ­nh ngáº¯n gá»n (1â€“2 dÃ²ng).
- TÃ³m Ã½ chÃ­nh thÃ nh 15â€“25 gáº¡ch Ä‘áº§u dÃ²ng, má»—i gáº¡ch báº¯t Ä‘áº§u báº±ng '-'.
- Xuáº¥t theo Ä‘á»‹nh dáº¡ng:
  Chá»§ Ä‘á»: <tiÃªu Ä‘á»>
  TÃ³m Ã½ chÃ­nh: <má»™t cÃ¢u>
  - bullet 1
  - bullet 2
VÄƒn báº£n thÃ´:
{chunk}
"""
    try:
        resp = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.6,
            max_tokens=3000
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"Lá»—i khi sinh chá»§ Ä‘á»: {e}")
        return ""

# -------------------------------
# 4. Lá»c trÃ¹ng láº·p (tiÃªu Ä‘á» tuyá»‡t Ä‘á»‘i + ná»™i dung fuzzy â‰¥90%)
# -------------------------------
def is_duplicate_fuzzy(new_block: dict, existing_blocks: list, content_threshold=0.9) -> bool:
    new_title = new_block["title"].strip().lower()
    new_content = new_block["content"].strip()

    for block in existing_blocks:
        old_title = block["title"].strip().lower()
        old_content = block["content"].strip()

        # TiÃªu Ä‘á» trÃ¹ng tuyá»‡t Ä‘á»‘i
        if new_title == old_title:
            logger.warning(f"âš ï¸ Block '{new_block['title']}' bá»‹ bá» qua (tiÃªu Ä‘á» trÃ¹ng).")
            return True

        # Ná»™i dung fuzzy
        ratio = SequenceMatcher(None, new_content, old_content).ratio()
        if ratio >= content_threshold:
            logger.warning(f"âš ï¸ Block '{new_block['title']}' bá»‹ bá» qua (ná»™i dung tÆ°Æ¡ng Ä‘á»“ng {ratio:.2f}).")
            return True

    return False

# -------------------------------
# 5. Pipeline chÃ­nh
# -------------------------------
def build_training_corpus(input_path="raw_long_input.txt",
                          corpus_path="training_corpus_clustered.json",
                          max_words=1200,
                          dry_run=False):
    logger.info(f"ğŸ“‚ Äá»c vÄƒn báº£n tá»«: {input_path}")
    raw = open(input_path, "r", encoding="utf-8").read()
    logger.info(f"ğŸ“„ Äá»™ dÃ i vÄƒn báº£n gá»‘c: {len(raw.split())} tá»«.")

    cleaned = clean_text(raw)
    chunks = split_text(cleaned, max_words=max_words)
    logger.info(f"âœ‚ï¸ Chia thÃ nh {len(chunks)} chunk.")

    # Gá»i API cho táº¥t cáº£ chunk
    parts = [process_chunk(ch) for ch in chunks if ch.strip()]
    if not parts:
        logger.info("âŒ KhÃ´ng sinh Ä‘Æ°á»£c chá»§ Ä‘á» nÃ o.")
        return

    # Äá»c corpus hiá»‡n cÃ³, náº¿u chÆ°a cÃ³ hoáº·c rá»—ng thÃ¬ khá»Ÿi táº¡o []
    existing_blocks = []
    if os.path.exists(corpus_path):
        with open(corpus_path, "r", encoding="utf-8") as f:
            content = f.read().strip()
            if content:
                try:
                    existing_blocks = json.loads(content)
                except json.JSONDecodeError:
                    logger.warning("âš ï¸ File corpus bá»‹ lá»—i JSON, khá»Ÿi táº¡o láº¡i rá»—ng.")
                    existing_blocks = []
            else:
                logger.info("â„¹ï¸ File corpus rá»—ng, khá»Ÿi táº¡o láº¡i rá»—ng.")
                existing_blocks = []
    else:
        logger.info("â„¹ï¸ ChÆ°a cÃ³ file corpus, sáº½ khá»Ÿi táº¡o má»›i.")
        existing_blocks = []

    logger.info(f"ğŸ“Š Corpus hiá»‡n cÃ³ {len(existing_blocks)} block.")

    new_blocks = []
    skipped = 0

    # Xá»­ lÃ½ táº¥t cáº£ káº¿t quáº£ tá»« API
    for p in parts:
        if not p:
            continue
        lines = p.splitlines()
        if not lines:
            continue

        # Sá»‘ thá»© tá»± chá»§ Ä‘á» liÃªn tá»¥c
        seq_num = len(existing_blocks) + len(new_blocks) + 1

        # Láº¥y tiÃªu Ä‘á»
        m_title = re.match(r"^Chá»§ Ä‘á»\s*:\s*(.+)$", lines[0])
        title = f"Chá»§ Ä‘á» {seq_num}: {m_title.group(1).strip()}" \
                if m_title else f"Chá»§ Ä‘á» {seq_num}: {lines[0].strip()}"

        # Láº¥y tÃ³m Ã½ chÃ­nh
        summary_line = ""
        for line in lines[1:]:
            if line.lower().startswith("tÃ³m Ã½ chÃ­nh"):
                summary_line = line.split(":", 1)[-1].strip()
                break

        if not summary_line and len(lines) > 1 and not lines[1].strip().startswith("-"):
            summary_line = lines[1].strip()
            logger.warning(f"âš ï¸ Summary fallback: dÃ¹ng dÃ²ng thá»© hai cho '{title}'")

        if not summary_line and any(ln.strip().startswith("-") for ln in lines):
            summary_line = next(ln.strip().lstrip("-").strip() for ln in lines if ln.strip().startswith("-"))
            logger.warning(f"âš ï¸ Summary fallback: dÃ¹ng bullet Ä‘áº§u tiÃªn cho '{title}'")

        # Láº¥y bullets
        bullets = "\n".join([ln for ln in lines if ln.strip().startswith("-")])

        block = {
            "title": title,
            "summary": summary_line,
            "content": bullets
        }

        if not is_duplicate_fuzzy(block, existing_blocks):
            new_blocks.append(block)
        else:
            skipped += 1

    if not new_blocks:
        logger.info("âŒ KhÃ´ng cÃ³ chá»§ Ä‘á» má»›i nÃ o Ä‘á»ƒ thÃªm.")
        return

    if dry_run:
        logger.info("ğŸ” Dry-run: chá»‰ in ra káº¿t quáº£, khÃ´ng ghi file.")
        print(json.dumps(new_blocks, ensure_ascii=False, indent=2))
    else:
        all_blocks = existing_blocks + new_blocks
        with open(corpus_path, "w", encoding="utf-8") as f:
            json.dump(all_blocks, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… ÄÃ£ thÃªm {len(new_blocks)} chá»§ Ä‘á» má»›i.")
        logger.info(f"âš ï¸ Bá» qua {skipped} block do trÃ¹ng láº·p.")
        logger.info(f"ğŸ“Š Tá»•ng sá»‘ chá»§ Ä‘á» hiá»‡n táº¡i: {len(all_blocks)}")

# -------------------------------
# CLI
# -------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="XÃ¢y dá»±ng training corpus Pháº­t há»c (JSON).")
    parser.add_argument("--input", default="raw_long_input.txt", help="File input thÃ´")
    parser.add_argument("--output", default="training_corpus_clustered.json", help="File corpus JSON")
    parser.add_argument("--max-words", type=int, default=1200, help="Sá»‘ tá»« tá»‘i Ä‘a má»—i chunk")
    parser.add_argument("--dry-run", action="store_true", help="Chá»‰ cháº¡y thá»­, khÃ´ng ghi file")
    args = parser.parse_args()

    build_training_corpus(
        input_path=args.input,
        corpus_path=args.output,
        max_words=args.max_words,
        dry_run=args.dry_run
    )