# main.py
import os
import re
import json
import logging
import traceback

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from openai import OpenAI

from memory import ConversationMemory
from composer import run_composition, read_json_corpus  # ƒë·ªìng b·ªô d√πng JSON corpus

# ---------------------------
# Kh·ªüi t·∫°o b·ªô nh·ªõ to√†n c·ª•c (RAM-only)
# ---------------------------
memory = ConversationMemory(max_length=100)

conversation_context = {
    "current_topic": None,
    "last_summary": "",
    "last_talk": ""
}

# ---------------------------
# Logging c·∫•u h√¨nh chi ti·∫øt
# ---------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("GenoraAI")

# ---------------------------
# Load bi·∫øn m√¥i tr∆∞·ªùng + OpenAI client
# ---------------------------
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("OPENAI_API_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh trong file .env")

client = OpenAI(api_key=api_key)

# ---------------------------
# Kh·ªüi t·∫°o FastAPI app
# ---------------------------
app = FastAPI(
    title="Genora AI",
    description="API chat v·ªõi Genora AI s·ª≠ d·ª•ng FastAPI + OpenAI",
    version="1.2.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # khi deploy th·∫≠t, n√™n gi·ªõi h·∫°n domain
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

if os.path.isdir("static"):
    app.mount("/static", StaticFiles(directory="static"), name="static")

# ---------------------------
# Model d·ªØ li·ªáu cho history v√† input
# ---------------------------
class HistoryItem(BaseModel):
    role: str
    content: str

class ChatInput(BaseModel):
    message: str
    history: list[HistoryItem] = []

# ---------------------------
# H√†m ti·ªán √≠ch qu·∫£n l√Ω l·ªãch s·ª≠
# ---------------------------
HISTORY_FILE = "chat_history.json"

def load_history():
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_history():
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(conversation_log, f, ensure_ascii=False, indent=2)

def create_summary(text: str, max_sentences: int = 2) -> str:
    sentences = re.split(r'(?<=[.!?])\s+', text.strip())
    summary = " ".join(sentences[:max_sentences])
    return summary if summary else text[:200] + "..."

# ---------------------------
# Kh·ªüi t·∫°o conversation_log t·ª´ file
# ---------------------------
conversation_log = load_history()
for msg in conversation_log:
    memory.add(msg["role"], msg["content"])

def log_message(role: str, content: str):
    conversation_log.append({"role": role, "content": content})

def generate_reply(message: str, history: list[HistoryItem], mode: str = "normal") -> str:
    if mode == "normal":
        system_prompt = (
            "B·∫°n l√† Genora AI, m·ªôt tr·ª£ l√Ω th√¥ng minh. "
            "Khi tr·∫£ l·ªùi c√¢u h·ªèi th∆∞·ªùng, h√£y t·ªïng h·ª£p ki·∫øn th·ª©c ph·ªï th√¥ng ch√≠nh x√°c, "
            "ph√¢n t√≠ch r√µ r√†ng, m·ªü r·ªông chi·ªÅu s√¢u minh tri·∫øt, v√† tr√¨nh b√†y d·ªÖ hi·ªÉu b·∫±ng ti·∫øng Vi·ªát. "
            "C√≥ th·ªÉ di·ªÖn gi·∫£i, ph√¢n t√≠ch th∆°, k·ªá, ƒëo·∫°n vƒÉn ng·∫Øn, ho·∫∑c ph√°p tho·∫°i ng·∫Øn, "
            "lu√¥n ƒë·∫£m b·∫£o tr·∫£ l·ªùi ch√≠nh x√°c, c√≥ v√≠ d·ª• minh h·ªça, v√† khuy·∫øn kh√≠ch th·ª±c h√†nh. "

            # Phong c√°ch & ƒë·ªô tin c·∫≠y
            "Gi·ªØ vƒÉn phong trang nh√£, kh√°ch quan, tr√°nh s√°o r·ªóng; ch·ªâ n√≥i ƒëi·ªÅu c·∫ßn thi·∫øt. "
            "∆Øu ti√™n t√≠nh ch√≠nh x√°c: khi ƒë·ªÅ c·∫≠p t√™n ri√™ng, ni√™n ƒë·∫°i, kh√°i ni·ªám ph·ªï th√¥ng, h√£y c·∫©n tr·ªçng tr∆∞·ªõc khi k·∫øt lu·∫≠n. "

            # X·ª≠ l√Ω th∆°/k·ªá/ƒëo·∫°n vƒÉn ng·∫Øn
            "N·∫øu n·ªôi dung l√† th∆°/k·ªá/ƒëo·∫°n vƒÉn ng·∫Øn: "
            "1) T√≥m l∆∞·ª£c √Ω ch√≠nh ng·∫Øn g·ªçn; "
            "2) Ph√¢n t√≠ch h√¨nh ·∫£nh, ·∫©n d·ª•, c·∫•u tr√∫c; "
            "3) R√∫t ra √Ω nghƒ©a th·ª±c h√†nh v√† li√™n h·ªá ƒë·ªùi s·ªëng hi·ªán ƒë·∫°i; "
            "4) N·∫øu ph√π h·ª£p, chi·∫øu soi th√™m d∆∞·ªõi g√≥c nh√¨n Ph·∫≠t h·ªçc nh∆∞ng kh√¥ng √©p bu·ªôc. "

            # T·ªï ch·ª©c c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi th∆∞·ªùng
            "T·ªï ch·ª©c c√¢u tr·∫£ l·ªùi theo c√°c m·ª•c ng·∫Øn (1‚Äì5 m·ª•c), m·ªói m·ª•c 2‚Äì5 c√¢u, "
            "d·ªÖ qu√©t, c√≥ v√≠ d·ª• ho·∫∑c t√¨nh hu·ªëng minh h·ªça. "
            "N·∫øu c√¢u h·ªèi y√™u c·∫ßu ƒë·ªãnh nghƒ©a, ƒë∆∞a ƒë·ªãnh nghƒ©a ng·∫Øn tr∆∞·ªõc r·ªìi m·ªõi m·ªü r·ªông. "

            # Tr√°nh l·∫∑p & ƒëa g√≥c nh√¨n
            "Tr√°nh l·∫∑p √Ω; m·ªói c√¢u ph·∫£i mang th√™m gi√° tr·ªã. "
            "N·∫øu c√≥ nhi·ªÅu c√°ch hi·ªÉu, n√™u c√°c kh·∫£ nƒÉng v√† ti√™u ch√≠ ph√¢n bi·ªát r√µ r√†ng. "
            "Khi ƒë∆∞a khuy·∫øn ngh·ªã, ∆∞u ti√™n c√°c b∆∞·ªõc nh·ªè, kh·∫£ thi, c√≥ th·ª© t·ª±. "

            # H√†nh vi khi ng∆∞·ªùi d√πng g√µ 'th√™m'
            "Khi ng∆∞·ªùi d√πng g√µ 'th√™m', h√£y m·ªü r·ªông ƒë√∫ng n·ªôi dung tr∆∞·ªõc ƒë√≥: "
            "ƒëi s√¢u c√°c ƒëi·ªÉm c·ªët l√µi, th√™m v√≠ d·ª•, ƒë·ªëi chi·∫øu v√† k·∫øt n·ªëi th·ª±c h√†nh; "
            "kh√¥ng t·∫°o ch·ªß ƒë·ªÅ m·ªõi, kh√¥ng l·∫∑p l·∫°i m·ªü ƒë·∫ßu."
        )
    else:  # mode == "phap_thoai"
        system_prompt = (
            "B·∫°n l√† Genora AI, tr·ª£ l√Ω Ph·∫≠t h·ªçc th√¥ng minh. "
            "Khi tr·∫£ l·ªùi, h√£y bi√™n so·∫°n th√†nh b√†i ph√°p tho·∫°i b·∫±ng ti·∫øng Vi·ªát, trang nghi√™m, r√µ r√†ng, d·ªÖ hi·ªÉu. "
            "C·∫•u tr√∫c g·ª£i √Ω:\n"
            "1) Ti√™u ƒë·ªÅ\n"
            "2) M·ªü b√†i (~150‚Äì200 t·ª´)\n"
            "3) Th√¢n b√†i: 10 ti·ªÉu m·ª•c (m·ªói m·ª•c ~400‚Äì500 t·ª´)\n"
            "4) K·∫øt lu·∫≠n (~300‚Äì500 t·ª´)\n"
            "Gi·ªØ ƒë√∫ng ch·ªß ƒë·ªÅ theo c√¢u h·ªèi ng∆∞·ªùi d√πng, tr√°nh lan man."
        )

    messages = [{"role": "system", "content": system_prompt}]
    if history:
        messages += [h.dict() for h in history]
    messages.append({"role": "user", "content": message})

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.7,
        max_tokens=6000
    )
    return resp.choices[0].message.content.strip()

# ---------------------------
# H√†m m·ªü r·ªông ph√°p tho·∫°i/ ph·∫£n h·ªìi tr∆∞·ªõc ƒë√≥
# ---------------------------
def extend_composition(context: dict, mode: str = "corpus") -> str:
    """
    M·ªü r·ªông ph√°p tho·∫°i ho·∫∑c ph·∫£n h·ªìi tr∆∞·ªõc ƒë√≥.
    - mode="corpus": m·ªü r·ªông ph√°p tho·∫°i t·ª´ corpus
    - mode="normal": m·ªü r·ªông ph·∫£n h·ªìi th∆∞·ªùng
    """
    if not context["last_talk"]:
        return "‚ö†Ô∏è Kh√¥ng c√≥ n·ªôi dung tr∆∞·ªõc ƒë√≥ ƒë·ªÉ m·ªü r·ªông."

    extended_input = (
        f"H√£y m·ªü r·ªông v√† ƒë√†o s√¢u th√™m n·ªôi dung ƒë√£ tr·∫£ l·ªùi tr∆∞·ªõc "
        f"v·ªÅ ch·ªß ƒë·ªÅ: {context['current_topic']}.\n\n"
        f"T√≥m t·∫Øt tr∆∞·ªõc ƒë√≥: {context['last_summary']}\n\n"
        f"N·ªôi dung tr∆∞·ªõc: {context['last_talk'][:500]}...\n\n"
        f"Gi·ªØ vƒÉn phong thu·∫ßn Ph·∫≠t h·ªçc, trang nghi√™m, ph√¢n t√≠ch minh tri·∫øt, "
        f"c√≥ v√≠ d·ª• th·ª±c ti·ªÖn v√† khuy·∫øn kh√≠ch th·ª±c h√†nh."
    )

    reply = generate_reply(extended_input, [HistoryItem(**h) for h in memory.get()])
    context["last_talk"] = reply
    context["last_summary"] = create_summary(reply)
    return reply

# ---------------------------
# Endpoint /chat
# ---------------------------
@app.post("/chat")
async def chat(input: ChatInput):
    """
    X·ª≠ l√Ω h·ªôi tho·∫°i ch√≠nh v·ªõi Genora AI.
    - "compose <ch·ªß ƒë·ªÅ>": bi√™n so·∫°n ph√°p tho·∫°i t·ª´ corpus JSON.
    - "compose th√™m": m·ªü r·ªông ph√°p tho·∫°i tr∆∞·ªõc ƒë√≥ t·ª´ corpus.
    - "th√™m": m·ªü r·ªông ph·∫£n h·ªìi tr∆∞·ªõc ƒë√≥ (nh√°nh th∆∞·ªùng).
    - C√¢u h·ªèi b√¨nh th∆∞·ªùng: sinh ph·∫£n h·ªìi m·ªõi b·∫±ng generate_reply.
    """
    try:
        text = input.message.strip()
        memory.add("user", text)
        log_message("user", text)

        lowered = text.lower()
        reply = ""

        # ---------------------------
        # Nh√°nh corpus (compose)
        # ---------------------------
        if lowered.startswith("compose") or lowered.startswith("t√≥m √Ω t·ª´ corpus"):
            parts = text.split(" ", 1)
            user_topic = parts[1].strip() if len(parts) > 1 else ""

            if user_topic.lower() == "th√™m":
                reply = extend_composition(conversation_context, mode="corpus")
                # c·∫≠p nh·∫≠t context sau khi m·ªü r·ªông
                conversation_context["last_talk"] = reply
                conversation_context["last_summary"] = create_summary(reply)
                # current_topic gi·ªØ nguy√™n
            elif not user_topic:
                reply = "‚ö†Ô∏è Vui l√≤ng nh·∫≠p ch·ªß ƒë·ªÅ sau t·ª´ kh√≥a 'compose'."
            else:
                result = run_composition(api_key=api_key, user_topic=user_topic)
                summary = result.get("summary", "")
                talk = result.get("talk", "")
                conversation_context["current_topic"] = user_topic
                conversation_context["last_summary"] = summary
                conversation_context["last_talk"] = talk
                reply = talk or "‚ö†Ô∏è Kh√¥ng th·ªÉ bi√™n so·∫°n ph√°p tho·∫°i t·ª´ corpus cho ch·ªß ƒë·ªÅ n√†y."

        # ---------------------------
        # Nh√°nh th∆∞·ªùng (generate_reply)
        # ---------------------------
        else:
            if lowered == "th√™m":
                reply = extend_composition(conversation_context, mode="normal")
                # c·∫≠p nh·∫≠t context sau khi m·ªü r·ªông
                conversation_context["last_talk"] = reply
                conversation_context["last_summary"] = create_summary(reply)
                # current_topic gi·ªØ nguy√™n
            else:
                reply = generate_reply(
                    input.message,
                    [HistoryItem(**h) for h in memory.get()],
                    mode="normal"
                )
                conversation_context["current_topic"] = text
                conversation_context["last_talk"] = reply
                conversation_context["last_summary"] = create_summary(reply)

        # ---------------------------
        # L∆∞u ph·∫£n h·ªìi assistant
        # ---------------------------
        memory.add("assistant", reply)
        log_message("assistant", reply)
        save_history()

        # Logging preview
        logger.info(f"Assistant reply preview: {reply[:200]}{'...' if len(reply) > 200 else ''}")

        return {
            "reply": reply,
            "history": memory.get(),
            "log": conversation_log,
            "context": conversation_context
        }

    except Exception as e:
        logger.error("Error in /chat endpoint")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"L·ªói khi x·ª≠ l√Ω y√™u c·∫ßu /chat: {str(e)}")

# ---------------------------
# C√°c endpoint qu·∫£n l√Ω l·ªãch s·ª≠
# ---------------------------
@app.get("/chat-history")
def get_chat_history():
    return {"history": memory.get(), "log": conversation_log}

@app.post("/clear")
def clear_memory():
    memory.clear()
    conversation_log.clear()
    save_history()
    return {"message": "ƒê√£ x√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i."}

@app.get("/status")
def get_status():
    return {"message_count": len(memory.get()), "log_count": len(conversation_log)}

@app.get("/last-user-message")
def get_last_user_message():
    last_msg = memory.last_user_message()
    return {"last_user_message": last_msg} if last_msg else {"message": "Ch∆∞a c√≥ tin nh·∫Øn n√†o t·ª´ user."}

@app.get("/last-assistant-reply")
def get_last_assistant_reply():
    last_reply = memory.last_assistant_reply()
    return {"last_assistant_reply": last_reply} if last_reply else {"message": "Ch∆∞a c√≥ ph·∫£n h·ªìi n√†o t·ª´ assistant."}

# ---------------------------
# Endpoint: corpus status (ƒë·ªìng b·ªô d√πng JSON, th·ªëng k√™ theo "ch·ªß ƒë·ªÅ")
# ---------------------------
@app.get("/corpus-status")
def corpus_status():
    """
    Tr·∫£ v·ªÅ th·ªëng k√™ t√¨nh tr·∫°ng corpus JSON:
    - total_topics: t·ªïng s·ªë block/ch·ªß ƒë·ªÅ trong corpus
    - num_non_empty_topics: s·ªë block c√≥ n·ªôi dung kh√¥ng r·ªóng
    - sample_topics: m·ªôt s·ªë ch·ªß ƒë·ªÅ m·∫´u (t·ªëi ƒëa 5)
    - preview_samples: m·ªôt s·ªë ƒëo·∫°n n·ªôi dung m·∫´u (t·ªëi ƒëa 5, m·ªói ƒëo·∫°n c·∫Øt g·ªçn 120 k√Ω t·ª± + '...')
    """
    try:
        corpus_file = os.path.join("data", "training_corpus_clustered.json")
        corpus_blocks = read_json_corpus(corpus_file)

        total_topics = len(corpus_blocks)
        non_empty_blocks = [b for b in corpus_blocks if (b.get("content") or "").strip()]
        num_non_empty_topics = len(non_empty_blocks)

        # Ch·ªß ƒë·ªÅ s∆° b·ªô: ∆∞u ti√™n tr∆∞·ªùng 'topic', fallback b·∫±ng content
        topics = []
        for b in non_empty_blocks[:1000]:  # gi·ªõi h·∫°n ƒë·ªÉ tr√°nh qu√° n·∫∑ng
            topic = (b.get("topic") or "").strip()
            if not topic:
                content = (b.get("content") or "").strip()
                topic = (content[:60] + "...") if content else ""
            topics.append(topic)

        # L·∫•y m·ªôt s·ªë ƒëo·∫°n preview n·ªôi dung (c·∫Øt g·ªçn + '...')
        preview_samples = [
            ((b.get("content") or "").strip()[:120] + "...")
            for b in non_empty_blocks[:5]
        ]

        # Logging ƒë·ªìng b·ªô v·ªõi composer.py
        percent_non_empty = (num_non_empty_topics / total_topics * 100) if total_topics > 0 else 0
        logger.info(
            f"üìä Corpus status: t·ªïng {total_topics} block, "
            f"{num_non_empty_topics} block c√≥ n·ªôi dung "
            f"({percent_non_empty:.2f}% tr√™n t·ªïng). "
            f"Sample topics hi·ªÉn th·ªã: {len(topics[:5])}"
        )

        return {
            "total_topics": total_topics,
            "num_non_empty_topics": num_non_empty_topics,
            "sample_topics": topics[:5],
            "preview_samples": preview_samples
        }

    except Exception as e:
        logger.error("Error in /corpus-status endpoint")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"L·ªói khi ƒë·ªçc corpus JSON: {str(e)}")

# N·∫øu mu·ªën ch·∫°y local b·∫±ng python main.py
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)

